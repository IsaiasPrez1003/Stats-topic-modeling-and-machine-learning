{
	"ID_code": [
		"O.A",
		"I.A",
		"J.B",
		"P.C",
		"K.C",
		"A.C",
		"A.G",
		"A.K",
		"K.K",
		"E.L",
		"C.MD",
		"S.M",
		"O.P",
		"T.P",
		"A.Q",
		"A.R",
		"E.R",
		"G.T",
		"S.T"
	],
	"Question 1": [
		"Error helps determine the strength of the experiment and the power to reject the null hypothesis.",
		"The role that error plays in statistical inference has to do with type one and type two error. Error helps us determine our null hypothesis. Meaning, did our results occur as a result of the independent variable? Or did it occur by chance, or error.",
		"Error plays a role in statistical inference by showing between and within group differences. Error also shows where the data differs. In stats, error is always involved and is a part of many aspects of stats.",
		"Error lets you know if the results of the study were purely due to chance. Error describes the difference WITHIN groups.",
		"Error allows us to gain knowledge on a population and what has statistical significance.",
		"The role that error plays in statistical inference is that an inference can be influenced by the data presented. And if there is error present in the data, the statistical inference wouldn't be accurate to the data. Fix the error, and the inference of the data would likely change.",
		"The chance of error can be decreased when the sample size is larger, when the sample size is smaller, an error has more of a chance of occurring.",
		"It allows people to determine how accurate a study may be and to make inferences and predictions based on how consistent the data is in an experiment.",
		"Error can help measure the random chance in an experiment.",
		"It weakens statistical inferences.",
		"Discovering error, also known as chance, can help to determine whether a researchers hypothesis can be accurate or not. A big value of error can lead to big statistical interference with the experiment while a small value of error means small statistical interference.",
		"Error affects the overall presentation of the IV effect in an experiment. Smaller error equal greater IV effect (this is because t-scores are calculated by dividing IV effect/error). Nuisance error may cause within-group differences that influence the overall statistical measures of a group (such as the group mean and standard deviation). While these may increase or decrease the percieved effect (or lack of effect) of the independent variable, they are likely to be deemed insignificant in the long run, as more samples are taken out of a population and more data is gathered. Confounds, however, are a type of error that cause between group differences, and may cause researchers to commit Type I and Type II errors. This occurs when confounds amplify or diminish the IV effect in an experiment, and because statistics alone is unable to detect the presence of a confound, researchers may mistake this as their hypothesis being proven/disproven.",
		"Error which is also known as chance shows the statistical significance.",
		"Error is used to determine causality in an experiment when the IV Effect may not have had a strong influence on the data. Error is any part of the results obtained that isn't directly caused by the IV. More specifically, when looking at the different t-test perspectives, we often fail to reject the null hypothesis (H0), meaning that the IV effect is just error. Larger error can be a result of a small sample size with a wide range in data. Also, we find error in our extraneous variables, which are additional variables in the experiment. This includes random error, aka nuisance variables and confounds. Nuisance variables are ok to become error as long as they have a non-systematic influence and even out in the long run. Confounds are hidden from stats and math. They have a systematic influence, causing between-group variability as well as ruin causality for the t-test, making this a bad type of error for stats. When making statistical inferences, it's important that you consider all these factors before making decisions such as rejecting the null hypothesis and accepting the researcher's perspective or vice versa. If you don't fully evaluate it, then you might get Type I and/or Type II errors.",
		"Error throws off the accuracy of your study.",
		"When it comes to statistical inference, error is inevitable. The larger your sample is, the smaller your error becomes.",
		"Error is used in statistical inference in order to draw a more accurate conclusion about what is being inferred. When error is likely, then the researcher can better infer if error caused the results of the statistics.",
		"Error causes an interference with the experiment due to the deviations from the mean. The error would then cause the T-value to either be statistically significant or not. Causing us to either reject or accept the null hypothesis based on the experiment even with error.",
		"Error is the way in which statistics are interfered with. This is because the error is a form of statistical interference as error interferes with the accuracy of statistics."
	],
	"Question 2": [
		"SE of Mean tells you how far the sample mean is from the main mean (population). SE of Difference is the difference between the two SE of Mean.",
		"SE of Mean: this term explains how far off a sample of a population is from the sample mean. This will help us decide how accurate the data represents a population. SE of Difference: The standard error of the differences of the mean tells us that the standard error of the mean is smaller than the SE of Different and it shows uncertainty.",
		"The SE of the Mean is found by dividing standard deviation by the square root of n. The SE of the Mean is to find the error between the SD and the number of samples. The SE of Difference is the difference of within groups. SE of Difference is found by adding SEM1 and SEM2, and squaring both of the SDs.",
		"Standard Error of the Mean shows how much the means of two different samples differ from each other. The Standard of error of the difference kind of does the same but instead your using the sqaure root of the SD/n and adding them for both samples.",
		"Standard Error of the mean is the error of the mean of a set of data, while SE of Difference calculates the difference between means.",
		"The Standard Error of the Mean, is the standard error of an individual mean of a sample. While the Standard Error of the Difference is in reference to the standard error of two samples.",
		"Standard Error of the Mean is the ratio between a samples standard deviation and the square root of the sample size. The standard error of the difference is SD1 squared over the sample size added to SD2 squared over the sample size and all of the square rooted.",
		"SE of Mean describes the variability within a data set. SE of Difference represents how far values are from the mean of the data set.",
		"SEM: Identifies the amount of average error in a set of data. SED: Measures the difference of error between 2 or more sets of data.",
		"SE of the mean is the difference between the mean of the control group and the mean of the experimental group. I don't know what SE of difference is.",
		"Standard error of the means refers to the error found between groups, Ex, Type A and Type B groups of data, while Standard Error of Difference refers to within group error.",
		"SE of Mean refers to a process that takes the means of multiple samples, calculates a mean of means from that data, and determines standard error by comparing each individual mean to the mean of means. SE of Difference refers to a process that takes the difference between the means of two different samples (the IV Effect) and....? (Don't know how to explain this one).",
		"SEM is the iv effect divided by the SD sqaured over the square root of n.",
		"Standard Error of Mean is a measure of the variability between samples means, which tells us that sample means will vary, whether the sample mean is lower or higher than the true mean. Simply, the SE of Mean is used to measure the accuracy of a sample mean in comparison to the true mean. To find the SE of Mean, you take the SD of the experiment group and divide it by the data's sample size. The Standard Error of the Differences between means (SDODBM) is a measure of the IV effect, specifically if it is real or not. We have the Researcher's Hypothesis and the Null Hypothesis, which are used after completing the t-test. Once the t-test is complete, you evaluate obtained t on the SDODBM scale/graph, and determine if you reject or fail to reject (accept) the null hypothesis. As I already mentioned, we use the SE of Difference in order to determine if the IV effect is real or not.",
		"The SE of Mean is how far off your results are from the mean. The SE of Difference is the difference between your data.",
		"SE of Mean compares the standard error of a mean within its own sample. SE of Difference compares means with different samples; compares the difference of each sample.",
		"The Standard Error of the Mean is the likely error associated with the mean of the data. The Standard Error of the Difference is the likely error associated with the difference between two separate means in sets of data.",
		"SE of Mean is the difference between a large sample of means based on one sample mean. (Level 2). SE of Difference - To be quite honest I'm not sure of the definitions.",
		"SE of the Mean: the differences between the means, is used to determine the difference of means and if there is error between the means (ex. if there was a really big difference between the two when there shouldn't be). SE of Difference: the standard error that occurs when comparing the differences between two values, accounts for error in values and data."
	],
	"Question 3": [
		"IV effect is the difference between the sample means. It indicates the error within the samples relative to the population mean.",
		"IV Effect: The effect that the independent variable has on our experiment. The higher the effect of the IV, the lower chance of error which is what we are looking for when using statistical inference.",
		"IV effect shows the difference between groups. IV effect is also part of finding t test. To find the IV effect it is M1-M2. It plays a role in statistical inference because it shows the difference in the two means of our experiment.",
		"The IV Effect is the difference between two means. M1-M2. The IV effect shows the difference BETWEEN groups.",
		"The IV Effect is the signifigance that the independent variable has in a set of data, and it shows how data may be changed when an independent variable is applied.",
		"The IV Effect is what effect the independent variable had on the results of research. The role that this plays on statistical inference is that, if the IV Effect is present and affected the outcome of what is being observed, the statistical inference would be that there was an IV Effect and that there is statistical significance.",
		"The IV effect is when you subtract the means from 2 samples. It shows the difference between the average of each sample.",
		"The IV effect is used to represent relationships between data and the causality between them. It allows for predictions and inference by providing an accurate representation of the relationship between data sets.",
		"IV Effect is the difference between both Means. Can be used to find the t-value in data.",
		"The IV effect is the difference in means. It tells us if the IV caused a change in DV.",
		"The IV effect is a measure of the difference between two groups of data. If there is a big IV effect then there is a big difference between the groups means, which could lead to statistical interference.",
		"The IV Effect is the percieved difference between the control group and the experimental group, caused by the presence of (or lack of) an independent variable. The independent variable is a variable that researchers manipulate in an experiment to see whether it is causing a certain condition. In statistical inference, if the percieved IV effect is deemed statistically significant, meaning it falls within the 5% of results that are unlikely to occur by chance, researchers will reject the null hypothesis, which means that their hypothesis (that the IV effect caused a difference) may be correct. On the other hand, it the percieved IV effect is not statistically significant, researchers must fail to reject the null hypothesis, which means they acknowlege that the results of their experiment were likely to occur by chance alone. Greater IV effects are more likely to be statistically significant; therefore the larger the IV Effect (if error remains small as well), the more likely researchers are to reject the null hypothesis.",
		"The iv effect is the difference between the two means.",
		"The IV effect is simply the difference between two sample's means. It determines between-group variability. The IV effect is one of the two factors which play a role in determining the variance within a sample. IV effect is used to find the obtained t value, and is used when making the final decision to reject or accept the null hypothesis. If an IV Effect is small, that means the data is more accurate, because you want the IV Effect to be as close to 0 as possible. When selecting a hypothesis, it can be said that the IV Effect is real if we reject the null, and if we fail to reject it, it can be said that the IV Effect was just error. Thus, the effect size can tell us how important the IV Effect is in an experiment.",
		"The IV effect helps to decide to accept or reject the null hypothesis.",
		"When it comes to statistical inference, the IV Effect shows how far a sample is from representing its population.",
		"The IV effect is the difference between means of the group that had the IV applied to it and the group that did not. Any significant difference between the two groups is likely to be caused by the IV and is therefore the IV effect. In statistical inference, the IV effect can fall in different zones which represent the likelihood of the effect being due to chance/error or being real.",
		"The IV effect is the different between the experimental group and the control group. The Iv effect plays a role in statistical inference because it shows how much variation occurs between groups. With this variation value we then calculate the T-value and it then allows us to make an inference on whether or not it is statistically significant. To be honest I was gonna study after this so if my answers are way off that's why.",
		"The IV effect is the independent variable effect or the way that the independent variable effects the data. For example, a dependent variable is likely not going to cause any major effects but the job of the independent variable is to be measured and determine its effects on the dependent variables. The IV Effect plays a role in statistical interference because it is a way that statistics become inferred with since the IV effect influences outcomes and data."
	]
}
	
	
	
	














	
	
	     
	 















  
	
	
	